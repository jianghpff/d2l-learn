{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dca9252",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 层和块\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。\n",
    "在这里，整个模型只有一个输出。\n",
    "注意，单个神经网络\n",
    "（1）接受一些输入；\n",
    "（2）生成相应的标量输出；\n",
    "（3）具有一组相关 *参数*（parameters），更新这些参数可以优化某目标函数。\n",
    "\n",
    "然后，当考虑具有多个输出的网络时，\n",
    "我们利用矢量化算法来描述整层神经元。\n",
    "像单个神经元一样，层（1）接受一组输入，\n",
    "（2）生成相应的输出，\n",
    "（3）由一组可调整参数描述。\n",
    "当我们使用softmax回归时，一个单层本身就是模型。\n",
    "然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。\n",
    "\n",
    "对于多层感知机而言，整个模型及其组成层都是这种架构。\n",
    "整个模型接受原始输入（特征），生成输出（预测），\n",
    "并包含一些参数（所有组成层的参数集合）。\n",
    "同样，每个单独的层接收输入（由前一层提供），\n",
    "生成输出（到下一层的输入），并且具有一组可调参数，\n",
    "这些参数根据从下一层反向传播的信号进行更新。\n",
    "\n",
    "事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。\n",
    "例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层，\n",
    "这些层是由*层组*（groups of layers）的重复模式组成。\n",
    "这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛\n",
    "的识别和检测任务 :cite:`He.Zhang.Ren.ea.2016`。\n",
    "目前ResNet架构仍然是许多视觉任务的首选架构。\n",
    "在其他的领域，如自然语言处理和语音，\n",
    "层组以各种重复模式排列的类似架构现在也是普遍存在。\n",
    "\n",
    "为了实现这些复杂的网络，我们引入了神经网络*块*的概念。\n",
    "*块*（block）可以描述单个层、由多个层组成的组件或整个模型本身。\n",
    "使用块进行抽象的一个好处是可以将一些块组合成更大的组件，\n",
    "这一过程通常是递归的，如 :numref:`fig_blocks`所示。\n",
    "通过定义代码来按需生成任意复杂度的块，\n",
    "我们可以通过简洁的代码实现复杂的神经网络。\n",
    "\n",
    "![多个层被组合成块，形成更大的模型](../img/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "从编程的角度来看，块由*类*（class）表示。\n",
    "它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，\n",
    "并且必须存储任何必需的参数。\n",
    "注意，有些块不需要任何参数。\n",
    "最后，为了计算梯度，块必须具有反向传播函数。\n",
    "在定义我们自己的块时，由于自动微分（在 :numref:`sec_autograd` 中引入）\n",
    "提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。\n",
    "\n",
    "在构造自定义块之前，(**我们先回顾一下多层感知机**)\n",
    "（ :numref:`sec_mlp_concise` ）的代码。\n",
    "下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层，\n",
    "然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9895e279",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0483, -0.0309, -0.0172,  0.0294,  0.0249, -0.0087,  0.0669, -0.0453,\n",
       "          0.0568,  0.0037],\n",
       "        [ 0.0468, -0.0351, -0.0095,  0.0210,  0.0363,  0.0091,  0.0758, -0.0414,\n",
       "          0.0345, -0.0110]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,128),nn.ReLU(),nn.Linear(128,256),nn.ReLU(),nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be949c0e",
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，\n",
    "层的执行顺序是作为参数传递的。\n",
    "简而言之，(**`nn.Sequential`定义了一种特殊的`Module`**)，\n",
    "即在PyTorch中表示一个块的类，\n",
    "它维护了一个由`Module`组成的有序列表。\n",
    "注意，两个全连接层都是`Linear`类的实例，\n",
    "`Linear`类本身就是`Module`的子类。\n",
    "另外，到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。\n",
    "这实际上是`net.__call__(X)`的简写。\n",
    "这个前向传播函数非常简单：\n",
    "它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce5ce8",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "## [**自定义块**]\n",
    "\n",
    "要想直观地了解块是如何工作的，最简单的方法就是自己实现一个。\n",
    "在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea84f7",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "1. 将输入数据作为其前向传播函数的参数。\n",
    "1. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。\n",
    "1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "1. 存储和访问前向传播计算所需的参数。\n",
    "1. 根据需要初始化模型参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572894df",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "在下面的代码片段中，我们从零开始编写一个块。\n",
    "它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。\n",
    "注意，下面的`MLP`类继承了表示块的类。\n",
    "我们的实现只需要提供我们自己的构造函数（Python中的`__init__`函数）和前向传播函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4ded2c3-ba10-4159-9aa3-ee107cfe35c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2428e-03,  2.8459e-01,  5.0686e-01,  1.2231e+00,  1.1874e+00,\n",
      "          7.6645e-01, -1.5853e+00, -6.5739e-01, -3.0423e-01,  1.6686e+00,\n",
      "          1.3798e+00, -5.7612e-01, -7.8438e-01, -1.1864e+00,  5.4801e-01,\n",
      "          1.5484e+00, -1.8074e+00, -8.0865e-01,  4.2287e-02, -4.3335e-01,\n",
      "         -6.6815e-01, -1.6636e+00, -1.6118e+00,  1.1031e+00, -1.4886e-01,\n",
      "          1.2846e+00,  5.1386e-02,  1.2328e-01,  2.3257e-01,  2.4134e-01,\n",
      "         -1.3338e+00,  1.3714e+00,  1.2931e+00,  2.7957e-01, -3.9674e-01,\n",
      "         -1.0274e+00,  2.1228e-01, -2.0505e+00,  9.9468e-01, -1.0496e+00,\n",
      "          3.9008e-01, -1.4731e+00,  7.0961e-01, -5.4908e-01, -7.4225e-03,\n",
      "          2.1366e-01, -7.4318e-01,  3.3523e-01, -1.5214e+00,  4.9902e-01,\n",
      "          1.3722e+00,  7.9975e-01, -1.3757e+00,  2.5931e-01,  8.0292e-01,\n",
      "          4.4481e-01, -1.2172e+00,  2.1820e-01, -1.3655e+00,  1.0976e+00,\n",
      "         -2.3687e+00,  3.8074e-01,  1.2337e+00,  1.7481e+00, -5.0257e-01,\n",
      "         -5.4816e-01,  1.4687e+00,  9.9684e-01,  4.5865e-01,  1.7662e+00,\n",
      "          4.7030e-01,  1.2514e+00,  1.2434e+00,  9.2099e-02,  1.1900e+00,\n",
      "          9.0742e-01,  4.2411e-01, -1.0048e+00, -3.5580e-01, -9.9764e-01,\n",
      "          1.5645e+00, -5.6021e-01, -2.7916e-01, -9.6426e-01,  9.6975e-01,\n",
      "         -5.2444e-01, -1.9434e+00, -1.6862e+00,  5.5744e-01,  5.4568e-01,\n",
      "         -4.8153e-01,  1.2083e+00,  1.8855e-02,  2.8666e-01,  1.4218e+00,\n",
      "          3.7716e-01,  3.0015e-01, -8.3481e-03, -1.7784e-02, -7.7026e-01],\n",
      "        [ 5.0982e-01,  8.2323e-02, -1.1075e+00, -3.3812e-02, -2.7668e-01,\n",
      "          1.2104e+00,  8.3774e-02,  2.1471e+00,  7.0479e-02,  6.6859e-01,\n",
      "          2.6122e+00, -1.0027e+00,  2.5463e-01,  1.4213e+00,  1.1200e+00,\n",
      "          3.8746e-01, -9.2484e-01, -6.8219e-01,  7.7657e-01,  7.7245e-01,\n",
      "         -8.2728e-01, -7.3695e-01,  4.2242e-01,  1.1381e+00,  1.0136e+00,\n",
      "         -3.7748e-01,  2.5980e+00, -1.2052e+00,  1.3987e+00, -1.4888e+00,\n",
      "         -1.2048e-01, -1.0188e-01,  1.1070e+00,  1.1614e-01, -5.3529e-02,\n",
      "          7.5021e-01,  5.2466e-01, -1.2640e+00, -2.2506e+00, -2.1009e+00,\n",
      "          6.0010e-01, -3.4738e-01, -8.5213e-01, -1.7519e+00,  5.0587e-01,\n",
      "         -1.3402e+00,  8.2988e-01,  6.7024e-01, -1.2333e+00,  6.5028e-01,\n",
      "          1.7012e+00,  1.9551e-01, -5.5744e-01, -5.3664e-01,  1.8330e+00,\n",
      "         -9.3819e-01, -7.2728e-01,  2.6405e-01, -4.4750e-01,  2.4943e+00,\n",
      "          1.6890e+00,  2.9801e-01, -8.3391e-01, -1.1984e-01,  7.3012e-02,\n",
      "          1.2374e-01,  1.1223e+00,  8.6541e-02,  5.3587e-01, -1.5846e+00,\n",
      "          7.0058e-01,  3.6500e-01, -2.0466e+00, -1.2995e+00,  2.2837e+00,\n",
      "         -3.7050e-01,  7.4834e-01, -4.1041e-01, -1.1898e+00,  4.4526e-01,\n",
      "          1.0225e+00, -1.9890e-01,  4.9613e-01,  1.6423e-01,  9.4539e-02,\n",
      "          4.4998e-01,  5.9535e-01, -1.3894e-01, -1.8882e+00, -2.7056e-01,\n",
      "         -4.7404e-01,  9.5074e-01, -7.6958e-01, -7.7623e-01, -2.2985e-01,\n",
      "         -1.3260e+00,  2.8994e-01, -6.5450e-01, -7.3251e-01,  6.2115e-01],\n",
      "        [-5.8608e-01, -1.9468e+00, -3.8675e-01,  7.1691e-01, -5.8696e-01,\n",
      "          1.9081e+00,  5.1622e-01, -9.9700e-01, -3.3689e-02,  1.2869e+00,\n",
      "         -1.1563e+00,  3.0282e-01, -1.2235e-01,  7.5026e-01,  1.3647e+00,\n",
      "          2.0544e-01,  8.2503e-01,  9.5702e-02, -8.3906e-01,  1.5904e+00,\n",
      "         -6.9838e-01, -1.1643e+00, -1.6164e+00,  8.9020e-01,  2.7917e-01,\n",
      "          1.0067e+00,  1.2898e+00,  2.3365e-01, -1.1092e+00, -5.2421e-01,\n",
      "          5.6447e-01,  9.0091e-01,  4.3915e-01,  1.7782e+00,  9.9152e-01,\n",
      "          6.2790e-01, -7.4672e-01, -8.3887e-01,  2.6423e-01, -2.5031e-01,\n",
      "         -9.2103e-01,  4.9455e-02,  7.4445e-01, -3.7365e-01, -1.7262e+00,\n",
      "         -2.0206e+00, -9.5016e-02, -1.3103e+00,  3.3150e-01, -1.3819e+00,\n",
      "          7.5085e-01, -6.9485e-01, -2.3741e-01,  9.3723e-01, -1.3538e+00,\n",
      "          9.7105e-01, -1.1209e+00, -3.3137e-01, -6.0265e-01,  9.4446e-01,\n",
      "          1.6354e+00, -2.0118e-01,  8.6545e-01, -8.4899e-02, -1.3385e-01,\n",
      "          9.3960e-01, -1.2433e+00, -1.2724e+00, -2.2993e+00, -5.2599e-02,\n",
      "         -3.7826e-01,  6.6861e-01, -7.7719e-01, -7.2633e-01, -1.2468e+00,\n",
      "          4.5358e-01, -8.4602e-02, -1.9621e+00,  9.1121e-01, -1.1203e+00,\n",
      "          8.9819e-01, -1.7483e+00, -4.4895e-01,  1.1643e-01,  1.4529e+00,\n",
      "         -6.4521e-01,  1.1795e+00, -2.3343e+00,  1.4278e+00,  6.4281e-01,\n",
      "          1.6255e+00, -1.6062e+00, -1.6381e+00, -1.7032e+00,  8.4419e-01,\n",
      "         -1.6542e+00, -1.6032e+00,  1.4796e+00,  1.0412e-01, -9.2634e-01],\n",
      "        [ 4.4101e-01,  9.9359e-01,  1.5726e+00,  1.7748e+00,  9.6554e-01,\n",
      "         -1.6121e-01,  6.1312e-01, -1.3832e-01, -1.2703e+00, -2.3091e-01,\n",
      "         -1.3840e-01, -3.9086e-01,  2.5160e+00,  1.1091e+00, -1.0676e-01,\n",
      "          1.3844e+00, -6.0349e-01,  6.8707e-01, -6.3559e-01,  2.6347e+00,\n",
      "         -8.3712e-01,  2.4444e-01, -1.3834e+00,  1.5642e-01,  8.4897e-01,\n",
      "         -5.3785e-01,  1.2725e+00, -8.4559e-01,  1.3004e+00,  7.3149e-01,\n",
      "          1.0685e-01,  8.6993e-02, -4.2800e-01,  6.5644e-01, -2.3870e-01,\n",
      "          2.9542e-01, -3.2691e-01, -2.9603e-01, -1.5025e+00,  4.1331e-01,\n",
      "          1.9057e-01, -1.9521e+00,  1.2863e-01,  2.0549e-01, -8.7760e-02,\n",
      "          1.0899e-01, -8.8413e-01,  2.4410e+00, -1.5365e-01, -1.3232e-01,\n",
      "         -7.5637e-01,  7.8231e-01,  6.0326e-02, -8.8290e-01, -7.4338e-01,\n",
      "         -9.1661e-01,  4.8229e-01,  2.0228e+00, -7.7756e-01, -3.8113e-01,\n",
      "         -2.1286e+00, -7.5150e-01,  1.5879e-01,  1.1498e+00,  1.3277e+00,\n",
      "          9.1991e-01, -1.7620e+00,  8.3686e-01,  1.6773e-01,  4.5949e-01,\n",
      "          5.4830e-01,  1.4252e+00,  5.4878e-01,  1.3407e+00,  1.0026e+00,\n",
      "         -1.6053e+00,  2.7586e-01,  3.1841e-01, -8.3991e-01, -9.7925e-02,\n",
      "         -4.4193e-01,  1.6262e-01, -5.7095e-01, -6.0605e-01,  5.7765e-01,\n",
      "         -4.9975e-02, -7.3584e-01, -8.6770e-02,  6.3795e-01, -1.0863e-01,\n",
      "         -7.7105e-01, -8.0117e-01, -3.9138e-01, -9.0138e-01, -7.8735e-02,\n",
      "         -2.5889e+00,  1.8493e+00, -3.4457e-01, -1.0540e+00, -3.3669e-01],\n",
      "        [ 5.5430e-01,  1.2857e-01,  6.5796e-01,  6.1094e-01, -1.7610e+00,\n",
      "          1.4654e-01, -9.2172e-03, -9.2541e-01,  7.7129e-01,  1.0238e+00,\n",
      "         -5.5296e-01, -4.8216e-01, -1.8761e+00,  1.5321e+00,  4.7898e-01,\n",
      "          1.4469e-01, -3.1572e-01,  1.1641e+00,  1.3665e+00, -2.4215e+00,\n",
      "         -7.3784e-01, -2.1691e+00, -2.1815e+00, -5.2125e-01,  6.6735e-01,\n",
      "         -8.7074e-01,  1.8748e+00, -7.1865e-01,  1.1075e+00,  1.5834e+00,\n",
      "         -2.1851e+00, -5.9220e-01,  2.1442e-01, -9.4508e-01, -1.2941e+00,\n",
      "         -8.0229e-01,  8.7711e-01, -3.3923e-01, -1.3575e+00,  3.7019e-01,\n",
      "          9.8652e-01,  8.5311e-01,  7.6407e-01, -6.1879e-01,  5.4173e-01,\n",
      "          7.4441e-01, -1.8518e+00,  4.7379e-01,  2.3035e-01,  1.5349e-01,\n",
      "          1.2907e+00, -8.2107e-01, -7.9522e-01,  1.6569e-01,  2.4837e-01,\n",
      "         -2.6904e-01, -1.3415e-01,  1.7552e-01, -5.0682e-02, -1.1088e-01,\n",
      "         -1.1483e+00,  2.2949e-01,  1.0731e+00,  1.7865e-01, -7.0003e-02,\n",
      "         -7.7974e-01,  1.2125e+00,  6.5089e-01, -4.9280e-01, -1.9476e+00,\n",
      "          1.2998e+00,  4.8048e-01, -6.8893e-01, -4.6235e-01, -7.6044e-01,\n",
      "         -7.5661e-01, -1.0616e+00,  9.4099e-01,  1.2412e+00, -8.4037e-01,\n",
      "         -6.8521e-01, -7.4854e-01,  2.0573e+00,  6.1788e-01,  3.0745e-01,\n",
      "          1.2931e+00, -1.3275e-01,  4.4628e-01, -2.6411e-01,  1.8612e+00,\n",
      "          7.3476e-01, -1.4621e+00,  1.8541e+00,  1.9768e-01, -5.2890e-03,\n",
      "          7.0258e-02,  7.8497e-01,  1.2745e+00, -6.4589e-01,  2.4481e-01],\n",
      "        [ 1.4021e+00, -7.7033e-02,  2.3182e-01,  4.7898e-01,  1.2269e-02,\n",
      "          1.4092e+00, -7.1045e-02,  3.2096e-02,  5.1649e-01, -1.8961e-01,\n",
      "          3.3041e-01,  1.0324e+00,  6.5135e-01, -8.4863e-01, -4.6296e-01,\n",
      "          8.6631e-01, -1.3077e+00, -1.9132e-01,  6.4691e-01,  2.2663e-01,\n",
      "          2.7716e-01, -2.4309e+00, -1.7431e+00,  1.3512e+00, -6.0045e-02,\n",
      "         -1.2021e-01, -3.7548e-01,  2.2885e+00, -1.0159e+00,  1.7863e-01,\n",
      "         -1.2765e+00, -1.3672e-01, -1.9997e-01,  7.5943e-01, -6.6598e-02,\n",
      "          2.9426e-01,  3.7731e-01,  3.2030e-01, -1.2690e+00, -1.1816e+00,\n",
      "          8.6230e-01,  4.6806e-01,  1.1709e+00, -1.3521e-01, -4.2137e-01,\n",
      "         -7.8434e-01,  4.1131e-01, -9.3969e-01, -2.4290e-01,  4.4035e-01,\n",
      "          5.1445e-01, -6.8927e-01, -1.9075e+00,  1.0190e+00,  7.5043e-02,\n",
      "          7.0592e-01, -5.1163e-02,  5.5402e-01,  6.6592e-01,  1.6181e+00,\n",
      "          1.4267e-01,  1.3219e+00, -1.7376e+00,  1.3886e+00, -2.0228e+00,\n",
      "          9.0417e-01,  1.7022e-01, -7.2685e-01,  1.2041e+00,  1.4699e-01,\n",
      "          1.8136e+00, -9.4680e-01, -2.4417e-01, -1.5344e-01, -4.9282e-01,\n",
      "          6.2564e-02,  3.3959e-01,  3.5798e-01, -7.2558e-01,  1.0879e+00,\n",
      "          1.3383e+00, -6.8181e-02,  6.5883e-01, -2.6563e+00, -2.2348e+00,\n",
      "         -2.3238e-01,  1.1084e+00,  1.2396e+00,  3.7062e-01, -2.2441e-01,\n",
      "          1.4168e+00,  1.1773e+00,  5.0613e-01,  3.0547e-01,  1.3518e-02,\n",
      "          1.1919e-01,  1.3964e-01,  1.0315e+00,  2.0050e+00, -2.5047e-01],\n",
      "        [-9.1613e-01,  2.4734e-02, -8.1670e-01, -1.2245e+00, -2.1057e+00,\n",
      "         -3.8071e-01, -5.5073e-01, -7.1548e-01,  1.2265e+00,  2.8126e-01,\n",
      "          1.3710e+00,  7.2252e-01, -9.0126e-01,  6.1354e-01, -1.6656e-01,\n",
      "         -2.1922e-01,  1.3705e+00,  7.2732e-01,  2.1241e-02, -2.5352e-01,\n",
      "          2.3500e-01, -8.6590e-01, -1.3320e+00,  1.5614e+00,  7.4623e-02,\n",
      "         -7.6608e-01,  1.0495e+00, -5.8195e-01,  4.6524e-01,  1.2318e+00,\n",
      "         -1.4562e-01, -6.3724e-01,  1.0024e+00, -1.2982e+00, -8.4431e-02,\n",
      "         -4.0343e-01, -7.9968e-02, -1.3345e-01,  1.8703e-01, -5.3665e-01,\n",
      "          9.2537e-01, -6.9874e-01, -4.1346e-01,  1.7984e-01,  4.4863e-01,\n",
      "          3.9821e-01,  1.0354e+00, -2.6351e-01,  6.9227e-01, -7.6430e-01,\n",
      "          8.3216e-01,  2.6260e-02, -5.4205e-01,  6.3006e-01, -4.9991e-01,\n",
      "         -2.8194e-01, -7.5780e-01, -6.2417e-01, -1.4603e+00,  7.0809e-01,\n",
      "          1.1402e+00,  5.4290e-01,  4.3102e-01, -1.0735e+00,  9.6003e-01,\n",
      "         -3.8343e-01, -1.1307e+00,  3.6670e+00,  2.0956e-01,  1.0241e+00,\n",
      "         -1.5164e+00, -1.1939e+00, -1.4117e-01,  1.0570e-01, -1.9665e+00,\n",
      "         -1.0529e+00,  1.6457e+00,  6.8203e-01,  7.3494e-01,  1.0934e+00,\n",
      "          7.0048e-01, -2.8940e-01,  1.1849e+00,  9.7542e-02, -4.4139e-01,\n",
      "          1.2006e+00,  1.1918e-01, -3.0284e-01,  6.4130e-01, -9.3500e-01,\n",
      "         -2.6599e-01, -7.2847e-01, -9.3859e-02,  2.9953e-01,  8.6927e-01,\n",
      "          7.4644e-01,  1.4421e-01, -1.2365e+00, -3.9627e-01,  7.1071e-01],\n",
      "        [ 3.5874e-01, -1.4312e+00,  1.5520e-02,  6.3209e-01,  1.6345e+00,\n",
      "         -1.5266e+00,  1.1928e+00,  6.2055e-01, -4.4596e-01,  6.7401e-01,\n",
      "          1.5578e+00, -6.1991e-01,  1.9589e+00,  6.4802e-01,  2.7718e-02,\n",
      "          8.2071e-01,  9.0428e-01, -3.9874e-01,  1.8617e-01,  1.3007e-01,\n",
      "          1.0290e+00,  6.5079e-02,  9.3789e-01,  3.2168e-01,  7.1447e-01,\n",
      "          1.2292e+00, -1.0335e+00,  2.7324e-01,  1.7906e+00,  1.4389e+00,\n",
      "         -9.3161e-01, -2.2166e-01,  9.5575e-01, -6.4780e-01,  3.2834e-01,\n",
      "         -8.0595e-02, -3.3700e-01,  1.0539e-01, -1.2010e+00,  1.8602e+00,\n",
      "         -1.6816e+00,  8.3829e-01, -1.6297e+00, -6.5942e-01, -2.4368e-01,\n",
      "          2.2003e-01,  2.2180e-01,  7.8135e-01, -8.2669e-01, -6.5655e-01,\n",
      "          1.9682e-01, -1.3804e+00, -4.6443e-02, -2.9777e-01, -2.2721e+00,\n",
      "         -6.5864e-01,  8.7787e-01,  1.8693e-01, -2.3767e+00, -7.3649e-01,\n",
      "          3.0224e-02, -7.6441e-01,  4.9826e-02, -3.7412e-01,  1.9317e+00,\n",
      "          1.0883e+00,  1.1908e-01, -1.1810e-01,  2.0512e-01, -6.9730e-01,\n",
      "          4.7484e-01, -6.3052e-01,  8.4604e-01, -1.5664e+00, -1.0650e+00,\n",
      "          1.0972e+00, -2.1029e-01,  1.2634e+00,  6.6254e-02, -1.9818e-01,\n",
      "          1.9625e+00, -3.3932e-01,  2.7621e-01,  3.7958e-01,  5.6388e-01,\n",
      "         -1.6506e+00,  5.3256e-01, -3.4180e-01, -2.6989e+00,  5.9288e-01,\n",
      "         -8.0129e-01, -4.0552e-01, -9.1722e-01,  1.4577e+00, -8.7812e-01,\n",
      "         -5.3465e-02,  9.2852e-01, -2.4020e-02, -4.6695e-01, -9.4446e-01],\n",
      "        [-6.5889e-01,  5.2216e-01,  2.3096e+00, -1.9100e-01,  3.8584e-01,\n",
      "          1.4192e+00, -5.3664e-01, -2.3455e+00,  4.4511e-01, -5.5846e-03,\n",
      "         -5.1663e-01, -1.1033e+00, -5.8546e-01, -5.8811e-01, -6.6693e-01,\n",
      "          5.1298e-01,  8.2052e-01,  1.7267e-01,  5.4123e-01, -7.1640e-01,\n",
      "         -4.9496e-01,  2.1012e+00,  1.0902e+00, -8.7023e-01,  6.5473e-01,\n",
      "          1.3154e-01, -2.3503e-01, -4.1201e-01,  5.3219e-01,  2.5516e-01,\n",
      "         -2.1459e+00, -1.4041e-02, -5.3169e-02,  9.7538e-01,  8.8716e-01,\n",
      "          1.8505e+00, -8.2370e-01,  4.3727e-01,  4.2611e-01, -1.6370e+00,\n",
      "         -1.1717e+00, -6.5527e-01, -9.8972e-01, -1.5934e+00,  2.0418e+00,\n",
      "          1.8549e+00,  4.0169e-01, -6.1405e-01, -4.6792e-01,  1.0581e+00,\n",
      "         -7.8970e-01, -9.3133e-02,  7.3545e-01, -8.2462e-01, -1.0459e-01,\n",
      "         -1.8339e+00, -2.2086e+00, -2.5082e-01,  2.0462e-01, -7.9839e-01,\n",
      "         -7.7755e-01,  1.1114e+00, -1.8785e+00,  3.0935e-01, -7.7058e-01,\n",
      "         -6.7785e-01, -4.5264e-01, -2.1661e+00, -2.0803e+00,  1.8181e+00,\n",
      "         -9.5417e-01,  2.5914e+00,  1.5541e+00, -1.5097e+00,  3.3829e-01,\n",
      "         -1.4117e-01,  4.7645e-01, -3.2904e-01, -6.5542e-01,  7.3355e-01,\n",
      "         -1.8226e+00,  1.8051e+00,  3.1629e-01,  2.8097e+00, -3.8583e-01,\n",
      "          7.2664e-01, -4.5165e-01,  6.6537e-01,  3.7577e-01,  2.4906e+00,\n",
      "          5.1267e-02,  7.7216e-01, -1.2246e+00, -1.4550e+00, -6.5907e-01,\n",
      "         -4.5562e-01,  9.8668e-01, -1.4435e+00,  1.0109e+00,  1.1599e+00],\n",
      "        [ 1.1346e+00, -9.1736e-01, -1.4297e+00, -6.0782e-01, -9.0159e-01,\n",
      "          1.2203e+00,  1.0633e+00, -1.4126e-01,  3.8195e+00, -8.4422e-01,\n",
      "          1.2657e+00, -1.5710e-01, -7.1467e-01,  2.0999e+00, -5.7299e-01,\n",
      "         -5.0111e-01, -2.0763e-01,  3.2285e-01, -5.0056e-01,  3.3756e-02,\n",
      "          1.1632e+00,  1.4565e-01,  9.1674e-01, -1.5113e+00, -1.8748e-01,\n",
      "         -4.8377e-01, -1.1221e-01, -1.2210e+00, -1.0339e+00,  1.1347e-01,\n",
      "          1.4291e+00,  4.5199e-01,  2.9638e-01,  1.4394e+00,  1.4136e-01,\n",
      "         -1.8343e+00, -5.7040e-01,  2.4410e+00, -1.5855e-01, -1.0440e+00,\n",
      "         -9.1294e-01,  1.1833e+00,  5.2083e-01, -5.2408e-02, -3.0267e-01,\n",
      "          8.1525e-01,  1.0029e+00,  1.6085e+00,  2.1600e-01,  6.7294e-02,\n",
      "         -1.6915e-01, -6.7285e-01,  1.3202e+00,  1.3097e+00, -5.5132e-01,\n",
      "          1.1009e-01,  1.1073e+00, -4.7218e-01, -1.9748e+00,  6.8478e-01,\n",
      "          3.4032e-01,  4.9828e-01, -1.8384e+00,  8.2445e-01, -5.2639e-02,\n",
      "          2.0760e-01, -3.2590e-01,  1.1222e+00, -9.5300e-01,  2.4834e+00,\n",
      "          9.8568e-01,  7.3098e-01,  2.7888e-01,  5.3919e-01, -3.0104e-01,\n",
      "          1.1468e+00, -8.1322e-01,  1.4542e-01,  6.6777e-01, -2.5287e-01,\n",
      "         -3.2303e+00, -2.0683e-01,  6.2534e-01,  3.6138e-01,  1.3723e+00,\n",
      "          2.8078e-01,  6.8334e-01, -3.2643e-01,  8.8852e-01, -6.4493e-01,\n",
      "          1.6826e+00, -2.8160e-01,  8.2633e-01, -2.5928e+00,  6.3501e-01,\n",
      "         -1.3956e+00,  1.1822e-01, -7.7992e-01,  1.4026e-01,  8.6559e-01]])\n",
      "After Seq\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5065, -0.1929,  1.6257, -0.1494, -0.9144,  0.4850,  0.7361, -0.7658,\n",
       "          0.4868, -0.1732],\n",
       "        [-0.5667, -0.2392,  1.5062, -0.0819, -0.5227,  0.6813,  0.4118, -0.1228,\n",
       "          0.2081, -0.1008],\n",
       "        [-0.3156, -0.2217,  2.0823,  0.3259, -0.7339,  0.6567,  0.5805,  0.1356,\n",
       "          0.1937, -0.4531],\n",
       "        [-0.3829, -0.1713,  2.0960,  0.0875, -0.7279,  0.3672,  0.9711, -0.4748,\n",
       "          0.4749, -0.3818],\n",
       "        [-0.4763, -0.0791,  1.9032, -0.2397, -0.8748,  0.6743,  0.7166, -0.7110,\n",
       "          0.2092, -0.2745],\n",
       "        [-0.8026, -0.3785,  2.0357, -0.0049, -0.8067,  0.6137,  0.6898, -0.4607,\n",
       "          0.3153, -0.1526],\n",
       "        [-0.4625, -0.0508,  1.9897, -0.1848, -0.6920,  0.6636,  0.6062, -0.2175,\n",
       "          0.0789, -0.1604],\n",
       "        [-0.6048, -0.3668,  1.8363, -0.3086, -0.6234,  0.8042,  0.7597, -0.5877,\n",
       "          0.6169, -0.3518],\n",
       "        [-0.3248, -0.7469,  1.8442, -0.1800, -1.2139,  0.6525,  1.0658, -0.1411,\n",
       "          0.2242, -0.1762],\n",
       "        [-0.5458, -0.5518,  2.1196,  0.1942, -1.1783,  0.4804,  0.7601, -0.5371,\n",
       "          0.6663, -0.2592]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myfirstmlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = torch.randn(10,20)\n",
    "        self.first = nn.Linear(100,128)\n",
    "        self.last = nn.Linear(128,10)\n",
    "        self.active =nn.ReLU()\n",
    "    def forward(self,X):\n",
    "        return self.last(self.active(self.first(X))+1)\n",
    "\n",
    "\n",
    "Y = torch.randn(10,100)\n",
    "net = myfirstmlp()\n",
    "print(Y)\n",
    "print('After Seq')\n",
    "net(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "876df867",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327a09c",
   "metadata": {
    "origin_pos": 17,
    "tags": []
   },
   "source": [
    "我们首先看一下前向传播函数，它以`X`作为输入，\n",
    "计算带有激活函数的隐藏表示，并输出其未规范化的输出值。\n",
    "在这个`MLP`实现中，两个层都是实例变量。\n",
    "要了解这为什么是合理的，可以想象实例化两个多层感知机（`net1`和`net2`），\n",
    "并根据不同的数据对它们进行训练。\n",
    "当然，我们希望它们学到两种不同的模型。\n",
    "\n",
    "接着我们[**实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层**]。\n",
    "注意一些关键细节：\n",
    "首先，我们定制的`__init__`函数通过`super().__init__()`\n",
    "调用父类的`__init__`函数，\n",
    "省去了重复编写模版代码的痛苦。\n",
    "然后，我们实例化两个全连接层，\n",
    "分别为`self.hidden`和`self.out`。\n",
    "注意，除非我们实现一个新的运算符，\n",
    "否则我们不必担心反向传播函数或参数初始化，\n",
    "系统将自动生成这些。\n",
    "\n",
    "我们来试一下这个函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7a34ec3",
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1084, -0.1816, -0.2111,  0.0839, -0.0767,  0.0301,  0.0320,  0.2233,\n",
       "          0.1416, -0.2566],\n",
       "        [-0.2014, -0.0316, -0.2136,  0.1048,  0.0470,  0.1435,  0.0534,  0.2596,\n",
       "          0.1832, -0.3165]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aaa7fc",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "块的一个主要优点是它的多功能性。\n",
    "我们可以子类化块以创建层（如全连接层的类）、\n",
    "整个模型（如上面的`MLP`类）或具有中等复杂度的各种组件。\n",
    "我们在接下来的章节中充分利用了这种多功能性，\n",
    "比如在处理卷积神经网络时。\n",
    "\n",
    "## [**顺序块**]\n",
    "\n",
    "现在我们可以更仔细地看看`Sequential`类是如何工作的，\n",
    "回想一下`Sequential`的设计是为了把其他模块串起来。\n",
    "为了构建我们自己的简化的`MySequential`，\n",
    "我们只需要定义两个关键函数：\n",
    "\n",
    "1. 一种将块逐个追加到列表中的函数；\n",
    "1. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n",
    "\n",
    "下面的`MySequential`类提供了与默认`Sequential`类相同的功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd09709c",
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "            print(type(self._modules))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3b379b6-9c44-45a8-98b0-ee47938874ae",
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.temp = []\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self.temp.append(module)\n",
    "            print(type(self.temp))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self.temp:\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44d091",
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "`__init__`函数将每个模块逐个添加到有序字典`_modules`中。\n",
    "读者可能会好奇为什么每个`Module`都有一个`_modules`属性？\n",
    "以及为什么我们使用它而不是自己定义一个Python列表？\n",
    "简而言之，`_modules`的主要优点是：\n",
    "在模块的参数初始化过程中，\n",
    "系统知道在`_modules`字典中查找需要初始化参数的子块。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272bce5",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "当`MySequential`的前向传播函数被调用时，\n",
    "每个添加的块都按照它们被添加的顺序执行。\n",
    "现在可以使用我们的`MySequential`类重新实现多层感知机。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9672de9a",
   "metadata": {
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0795,  0.2499, -0.0244,  0.1524,  0.1594,  0.0082,  0.0242, -0.0944,\n",
       "         -0.0341,  0.0689],\n",
       "        [-0.1358,  0.1801, -0.0378,  0.1745,  0.1251, -0.0010,  0.1291,  0.0102,\n",
       "          0.0717,  0.0507]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189aa472",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "请注意，`MySequential`的用法与之前为`Sequential`类编写的代码相同\n",
    "（如 :numref:`sec_mlp_concise` 中所述）。\n",
    "\n",
    "## [**在前向传播函数中执行代码**]\n",
    "\n",
    "`Sequential`类使模型构造变得简单，\n",
    "允许我们组合新的架构，而不必定义自己的类。\n",
    "然而，并不是所有的架构都是简单的顺序架构。\n",
    "当需要更强的灵活性时，我们需要定义自己的块。\n",
    "例如，我们可能希望在前向传播函数中执行Python的控制流。\n",
    "此外，我们可能希望执行任意的数学运算，\n",
    "而不是简单地依赖预定义的神经网络层。\n",
    "\n",
    "到目前为止，\n",
    "我们网络中的所有操作都对网络的激活值及网络的参数起作用。\n",
    "然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，\n",
    "我们称之为*常数参数*（constant parameter）。\n",
    "例如，我们需要一个计算函数\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，\n",
    "其中$\\mathbf{x}$是输入，\n",
    "$\\mathbf{w}$是参数，\n",
    "$c$是某个在优化过程中没有更新的指定常量。\n",
    "因此我们实现了一个`FixedHiddenMLP`类，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad09596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.364000Z",
     "iopub.status.busy": "2023-08-18T06:57:01.363468Z",
     "iopub.status.idle": "2023-08-18T06:57:01.369665Z",
     "shell.execute_reply": "2023-08-18T06:57:01.368755Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06017344",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "在这个`FixedHiddenMLP`模型中，我们实现了一个隐藏层，\n",
    "其权重（`self.rand_weight`）在实例化时被随机初始化，之后为常量。\n",
    "这个权重不是一个模型参数，因此它永远不会被反向传播更新。\n",
    "然后，神经网络将这个固定层的输出通过一个全连接层。\n",
    "\n",
    "注意，在返回输出之前，模型做了一些不寻常的事情：\n",
    "它运行了一个while循环，在$L_1$范数大于$1$的条件下，\n",
    "将输出向量除以$2$，直到它满足条件为止。\n",
    "最后，模型返回了`X`中所有项的和。\n",
    "注意，此操作可能不会常用于在任何实际任务中，\n",
    "我们只展示如何将任意代码集成到神经网络计算的流程中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ebc567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.373508Z",
     "iopub.status.busy": "2023-08-18T06:57:01.372789Z",
     "iopub.status.idle": "2023-08-18T06:57:01.380049Z",
     "shell.execute_reply": "2023-08-18T06:57:01.379025Z"
    },
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1862, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b18eb2",
   "metadata": {
    "origin_pos": 41
   },
   "source": [
    "我们可以[**混合搭配各种组合块的方法**]。\n",
    "在下面的例子中，我们以一些想到的方法嵌套块。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca3b399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.384091Z",
     "iopub.status.busy": "2023-08-18T06:57:01.383236Z",
     "iopub.status.idle": "2023-08-18T06:57:01.394649Z",
     "shell.execute_reply": "2023-08-18T06:57:01.393535Z"
    },
    "origin_pos": 43,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2183, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12e280",
   "metadata": {
    "origin_pos": 46
   },
   "source": [
    "## 效率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26229d3",
   "metadata": {
    "origin_pos": 48,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "读者可能会开始担心操作效率的问题。\n",
    "毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、\n",
    "代码执行和许多其他的Python代码。\n",
    "Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock)\n",
    "是众所周知的。\n",
    "在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa617e6",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "* 块可以包含代码。\n",
    "* 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "* 层和块的顺序连接由`Sequential`块处理。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果将`MySequential`中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    "1. 实现一个块，它以两个块为参数，例如`net1`和`net2`，并返回前向传播中两个网络的串联输出。这也被称为平行块。\n",
    "1. 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7de2b31-3371-431d-b4f2-0448a666858b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class seq1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(128,256)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.input(X)\n",
    "\n",
    "class seq2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(256,128)\n",
    "        self.ac = nn.ReLU()\n",
    "        self.output = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return  self.output(self.ac(self.input(X)))\n",
    "\n",
    "    \n",
    "class myseq3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = seq1()\n",
    "        self.output = seq2()\n",
    "    def forward(self,X):\n",
    "        return seq1(seq2(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "459aff57-c6c2-44ef-8e5c-a17054a02259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0316,  0.0948,  0.1135,  0.0820,  0.0160,  0.1313,  0.0034,  0.0908,\n",
       "          0.0059, -0.0061],\n",
       "        [-0.0891,  0.0282,  0.2301,  0.1031,  0.1220,  0.2257,  0.0274,  0.0113,\n",
       "          0.1156,  0.1176]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(2,128)\n",
    "net = nn.Sequential(seq1(),nn.ReLU(),seq2())\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29846c8",
   "metadata": {
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1827)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f9c799e-4596-4860-a7aa-9a03d7fb7895",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x10 and 64x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m large_network \u001b[38;5;241m=\u001b[39m build_large_network(num_blocks, input_dim, hidden_dim, output_dim)\n\u001b[1;32m     38\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mlarge_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 打印网络结构\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(large_network)\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x10 and 64x128)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def build_network_block(input_dim, hidden_dim, output_dim):\n",
    "    \"\"\"\n",
    "    构建一个简单的网络块，该块包括一个线性层和一个激活函数。\n",
    "    \"\"\"\n",
    "    block = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, output_dim)\n",
    "    )\n",
    "    return block\n",
    "\n",
    "def build_large_network(num_blocks, input_dim, hidden_dim, output_dim):\n",
    "    \"\"\"\n",
    "    生成多个相同块的实例，并将它们连接在一起构建更大的网络。\n",
    "    \"\"\"\n",
    "    blocks = []  # 存储多个网络块的列表\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        block = build_network_block(input_dim, hidden_dim, output_dim)\n",
    "        blocks.append(block)\n",
    "\n",
    "    # 使用nn.Sequential将多个块连接在一起\n",
    "    network = nn.Sequential(*blocks)\n",
    "\n",
    "    return network\n",
    "\n",
    "# 使用示例\n",
    "input_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = 10\n",
    "num_blocks = 3  # 假设要构建三个相同块的网络\n",
    "\n",
    "# 构建具有多个块的更大网络\n",
    "large_network = build_large_network(num_blocks, input_dim, hidden_dim, output_dim)\n",
    "\n",
    "X = torch.randn(2,64)\n",
    "large_network(X)\n",
    "\n",
    "# 打印网络结构\n",
    "print(large_network)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
